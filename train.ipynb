{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training V05\n",
    "To be filled later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO> TensorFlow version: 1.14.0\n",
      "INFO> Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import os\n",
    "import subprocess\n",
    "import datetime\n",
    "import numpy\n",
    "import sklearn.metrics\n",
    "import tensorflow\n",
    "\n",
    "print(\"INFO> TensorFlow version: %s\" % tensorflow.__version__)\n",
    "print(\"INFO> Num GPUs Available: \", len(tensorflow.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO> Reading file config.yam from directory: /raid5/disk1/mlproj10/classification\n",
      "INFO> batchSize      : 16\n",
      "INFO> checkDataset   : True\n",
      "INFO> checkpointDir  : /home/jmv/data/mlproj10/tmp/\n",
      "INFO> createDataset  : True\n",
      "INFO> datasetDir     : /home/jmv/data/mlproj10/dataset\n",
      "INFO> goldenDataset  : /home/jmv/data/mlproj10/LeryPosesGolden/dataset\n",
      "INFO> imageHeight    : 720\n",
      "INFO> imageWidth     : 1280\n",
      "INFO> learningRate   : 1e-6\n",
      "INFO> logDir         : /home/jmv/data/mlproj10/log/\n",
      "INFO> nEpochs        : 1000\n",
      "INFO> nTrnSamples    : 2000\n",
      "INFO> nTstSamples    : 671\n",
      "INFO> nValSamples    : 671\n",
      "INFO> remDir         : /home/jmv/data/mlproj10/dataset/rem\n",
      "INFO> tmpDir         : /home/jmv/data/mlproj10/dataset/tmp\n",
      "INFO> trnDir         : /home/jmv/data/mlproj10/dataset/trn\n",
      "INFO> tstDir         : /home/jmv/data/mlproj10/dataset/tst\n",
      "INFO> valDir         : /home/jmv/data/mlproj10/dataset/val\n"
     ]
    }
   ],
   "source": [
    "# Read parameters from local config.yaml file, and update corresponding Python variables\n",
    "currentDir = os.getcwd()\n",
    "print(\"INFO> Reading file config.yam from directory: %s\" % currentDir)\n",
    "yamlFile = open('config.yaml','r')\n",
    "yamlData = yaml.load(yamlFile,Loader=yaml.Loader)\n",
    "\n",
    "for key in sorted(yamlData):\n",
    "    print(\"INFO> %-15s: %s\" % (key,yamlData[key]))\n",
    "\n",
    "batchSize = yamlData['batchSize']\n",
    "checkDataset = yamlData['checkDataset']\n",
    "checkpointDir = yamlData['checkpointDir']\n",
    "createDataset = yamlData['createDataset']\n",
    "datasetDir = yamlData['datasetDir']\n",
    "goldenDataset = yamlData['goldenDataset']\n",
    "imageHeight = yamlData['imageHeight']\n",
    "imageWidth = yamlData['imageWidth']\n",
    "learningRate = float(yamlData['learningRate'])\n",
    "logDir = yamlData['logDir']\n",
    "nEpochs = yamlData['nEpochs']\n",
    "nTrnSamples = yamlData['nTrnSamples']\n",
    "nTstSamples = yamlData['nTstSamples']\n",
    "nValSamples = yamlData['nValSamples']\n",
    "remDir = yamlData['remDir']\n",
    "tmpDir = yamlData['tmpDir']\n",
    "trnDir = yamlData['trnDir']\n",
    "tstDir = yamlData['tstDir']\n",
    "valDir = yamlData['valDir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder (1st pass): 0windsurf\n",
      "Processing folder (1st pass): 1windsurf\n",
      "Processing folder (2nd pass): 0windsurf\n",
      "Processing folder (2nd pass): 1windsurf\n",
      "INFO> Processing folder: rem\n",
      "INFO>     Number of samples in class 0windsurf: 414\n",
      "INFO>     Number of samples in class 1windsurf: 16\n",
      "INFO> Processing folder: tmp\n",
      "INFO>     Number of samples in class 0windsurf: 3756\n",
      "INFO>     Number of samples in class 1windsurf: 3358\n",
      "INFO> Processing folder: trn\n",
      "INFO>     Number of samples in class 0windsurf: 2000\n",
      "INFO>     Number of samples in class 1windsurf: 2000\n",
      "INFO> Processing folder: tst\n",
      "INFO>     Number of samples in class 0windsurf: 671\n",
      "INFO>     Number of samples in class 1windsurf: 671\n",
      "INFO> Processing folder: val\n",
      "INFO>     Number of samples in class 0windsurf: 671\n",
      "INFO>     Number of samples in class 1windsurf: 671\n"
     ]
    }
   ],
   "source": [
    "# Optionally run scripts to create dataset from golden data set & check the newly created dataset\n",
    "if createDataset:\n",
    "    # Remove datasetDir folder & sub-folders - and then create it again to start from scratch\n",
    "    shutil.rmtree(datasetDir, ignore_errors=True)\n",
    "    os.mkdir(datasetDir)\n",
    "    \n",
    "    # For each class from goldenDataset:\n",
    "    #  - create a corresponding subfolder in tmpDir\n",
    "    #  - copy all images from the corresponding class in goldenDataset to the corresponding class in tmpDir\n",
    "    listOfSubDirs = [subdir[1] for subdir in os.walk(goldenDataset)][0]\n",
    "    for myClass in listOfSubDirs:\n",
    "        print(f\"Processing folder (1st pass): {myClass}\")\n",
    "        os.makedirs(os.path.join(tmpDir,myClass))\n",
    "\n",
    "        for myImage in [image[2] for image in os.walk(os.path.join(goldenDataset,myClass))][0]:\n",
    "            shutil.copyfile(os.path.join(goldenDataset,myClass,myImage),os.path.join(tmpDir,myClass,myImage))\n",
    "        os.makedirs(os.path.join(trnDir,myClass))\n",
    "        os.makedirs(os.path.join(valDir,myClass))\n",
    "        os.makedirs(os.path.join(tstDir,myClass))\n",
    "        os.makedirs(os.path.join(remDir,myClass))\n",
    "\n",
    "    # Oversampling section\n",
    "    def overSample(myList,actualLength,desiredLength): \n",
    "        if actualLength==desiredLength:\n",
    "            return myList\n",
    "        else:\n",
    "            newList = numpy.append(myList,numpy.random.choice(myList,desiredLength-actualLength)) \n",
    "            return numpy.random.choice(newList,desiredLength,replace=False)\n",
    "    \n",
    "    def copyImages(tmpDir, subDir, listOfImages, dstDir):\n",
    "        cntDict={}\n",
    "        for image in listOfImages:\n",
    "            cntDict[image] = cntDict.get(image,0)+1\n",
    "            if cntDict[image]==1:\n",
    "                dstImage = image\n",
    "            else:\n",
    "                fileName, fileExtension = os.path.splitext(image)\n",
    "                dstImage = fileName+\"_copy_\"+str(cntDict[image])+fileExtension\n",
    "            shutil.copyfile(os.path.join(tmpDir,subDir,image),os.path.join(dstDir,subDir,dstImage))\n",
    "        \n",
    "    random.seed(42)\n",
    "    \n",
    "    listOfSubDir = [subdir[1] for subdir in os.walk(tmpDir)][0]\n",
    "    for subDir in listOfSubDir:\n",
    "        print(\"Processing folder (2nd pass): %s\" % subDir)\n",
    "    \n",
    "        initialList=os.listdir(os.path.join(tmpDir,subDir))\n",
    "        nInitialList = len(initialList)\n",
    "        randomInitialList = random.sample(initialList,k=nInitialList)\n",
    "        \n",
    "        # Split randomized list of images according to the training, validation & test ratio\n",
    "        nTotalSamples = nTrnSamples + nValSamples + nTstSamples\n",
    "        trnN = min(nTrnSamples,int(nInitialList*nTrnSamples/nTotalSamples))\n",
    "        valN = min(nTrnSamples+nValSamples,int(nInitialList*(nTrnSamples+nValSamples)/nTotalSamples))\n",
    "        tstN = min(nTotalSamples,nInitialList)\n",
    "        trn, val, tst, rem = numpy.split(randomInitialList,[trnN, valN, tstN])\n",
    "    \n",
    "        # Oversampling scheme\n",
    "        trnLen = trnN\n",
    "        valLen = valN-trnN\n",
    "        tstLen = tstN-valN\n",
    "        fullTrn = overSample(trn,trnLen,nTrnSamples)\n",
    "        fullVal = overSample(val,valLen,nValSamples)\n",
    "        fullTst = overSample(tst,tstLen,nTstSamples)\n",
    "        fullRem = rem # No oversampling for rem\n",
    "    \n",
    "        # Copy images in folders\n",
    "        copyImages(tmpDir, subDir, fullTrn, trnDir)\n",
    "        copyImages(tmpDir, subDir, fullVal, valDir)\n",
    "        copyImages(tmpDir, subDir, fullTst, tstDir)        \n",
    "        copyImages(tmpDir, subDir, fullRem, remDir)   \n",
    "    \n",
    "if checkDataset:\n",
    "    for subDir in sorted(os.listdir(datasetDir)):\n",
    "        print(f\"INFO> Processing folder: {subDir}\")\n",
    "        for myClass in sorted(os.listdir(os.path.join(datasetDir, subDir))):\n",
    "            nImages = len(os.listdir(os.path.join(datasetDir, subDir, myClass)))\n",
    "            print(f\"INFO>     Number of samples in class {myClass}: {nImages}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /raid5/disk1/tensorflow1.14/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 720, 1280, 3)]    0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 718, 1278, 64)     1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 718, 1278, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 359, 639, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 357, 637, 128)     73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 357, 637, 128)     512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 178, 318, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 176, 316, 192)     221376    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 176, 316, 192)     768       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 88, 158, 192)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 86, 156, 192)      331968    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 86, 156, 192)      768       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 43, 78, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 41, 76, 192)       331968    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 41, 76, 192)       768       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 20, 38, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 18, 36, 128)       221312    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 18, 36, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 9, 18, 128)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 20736)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               2654336   \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 3,840,833\n",
      "Trainable params: 3,838,785\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /raid5/disk1/tensorflow1.14/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "myModelInput = tensorflow.keras.layers.Input(shape=(imageHeight,imageWidth,3))\n",
    "x = tensorflow.keras.layers.Conv2D(64, (3,3), activation=\"relu\")(myModelInput)\n",
    "x = tensorflow.keras.layers.BatchNormalization()(x)\n",
    "x = tensorflow.keras.layers.MaxPooling2D((2,2))(x)\n",
    "\n",
    "x = tensorflow.keras.layers.Conv2D(128, (3,3), activation=\"relu\")(x)\n",
    "x = tensorflow.keras.layers.BatchNormalization()(x)\n",
    "x = tensorflow.keras.layers.MaxPooling2D((2,2))(x)\n",
    "\n",
    "x = tensorflow.keras.layers.Conv2D(192, (3,3), activation=\"relu\")(x)\n",
    "x = tensorflow.keras.layers.BatchNormalization()(x)\n",
    "x = tensorflow.keras.layers.MaxPooling2D((2,2))(x)\n",
    "\n",
    "x = tensorflow.keras.layers.Conv2D(192, (3,3), activation=\"relu\")(x)\n",
    "x = tensorflow.keras.layers.BatchNormalization()(x)\n",
    "x = tensorflow.keras.layers.MaxPooling2D((2,2))(x)\n",
    "\n",
    "x = tensorflow.keras.layers.Conv2D(192, (3,3), activation=\"relu\")(x)\n",
    "x = tensorflow.keras.layers.BatchNormalization()(x)\n",
    "x = tensorflow.keras.layers.MaxPooling2D((2,2))(x)\n",
    "\n",
    "x = tensorflow.keras.layers.Conv2D(128, (3,3), activation=\"relu\")(x)\n",
    "x = tensorflow.keras.layers.BatchNormalization()(x)\n",
    "x = tensorflow.keras.layers.MaxPooling2D((2,2))(x)\n",
    "\n",
    "x = tensorflow.keras.layers.Flatten()(x)\n",
    "x = tensorflow.keras.layers.Dense(128, activation=\"relu\")(x)\n",
    "x = tensorflow.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "x = tensorflow.keras.layers.Dropout(0.25)(x)\n",
    "myModelOutput = tensorflow.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = tensorflow.keras.models.Model(inputs=myModelInput, outputs=myModelOutput)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "tensorflow.keras.optimizers.RMSprop(lr=learningRate)\n",
    " \n",
    "model.compile(loss=tensorflow.keras.losses.BinaryCrossentropy(),\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc']) # should be accuracy in TF2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4000 images belonging to 2 classes.\n",
      "Found 1342 images belonging to 2 classes.\n",
      "Epoch 1/1000\n",
      " 53/125 [===========>..................] - ETA: 28:13 - loss: 0.8763 - acc: 0.6285"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-60655f0c6456>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnValSamples\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatchSize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     callbacks=[tensorboardCallback,checkpoint])\n\u001b[0m",
      "\u001b[0;32m/raid5/disk1/tensorflow1.14/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m/raid5/disk1/tensorflow1.14/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/raid5/disk1/tensorflow1.14/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1173\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/raid5/disk1/tensorflow1.14/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m/raid5/disk1/tensorflow1.14/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trnDataGen = tensorflow.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "valDataGen = tensorflow.keras.preprocessing.image.ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "trnGenerator = trnDataGen.flow_from_directory(\n",
    "    trnDir,\n",
    "    target_size=(imageHeight,imageWidth),\n",
    "    batch_size=batchSize,\n",
    "    class_mode='binary')\n",
    "\n",
    "valGenerator = valDataGen.flow_from_directory(\n",
    "    valDir,\n",
    "    target_size=(imageHeight,imageWidth),\n",
    "    batch_size=batchSize,\n",
    "    class_mode='binary')\n",
    "\n",
    "timeNow = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "fullCheckpointDir = os.path.join(checkpointDir,timeNow)\n",
    "os.mkdir(fullCheckpointDir)\n",
    "                                           \n",
    "# need to replace acc by accuracy below when moving to TF2.0    \n",
    "filePath = os.path.join(fullCheckpointDir,\"{epoch:05d}_{loss:.6f}_{acc:.6f}_{val_loss:.6f}_{val_acc:.6f}.h5\")\n",
    "checkpoint = tensorflow.keras.callbacks.ModelCheckpoint(filePath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', save_freq='epoch')\n",
    "\n",
    "# profile_batch=0 required to solve a bug w/ Tensorboard according to     \n",
    "#   https://github.com/tensorflow/tensorboard/issues/2412    \n",
    "fullLogDir = os.path.join(logDir, timeNow)\n",
    "tensorboardCallback = tensorflow.keras.callbacks.TensorBoard(log_dir=fullLogDir,profile_batch=0)\n",
    "\n",
    "history = model.fit_generator(\n",
    "    trnGenerator,\n",
    "    steps_per_epoch=nTrnSamples // batchSize,\n",
    "    epochs=nEpochs,\n",
    "    validation_data=valGenerator,\n",
    "    validation_steps=nValSamples // batchSize,\n",
    "    callbacks=[tensorboardCallback,checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look in the tmp directory and select best model candidate based on train/val loss & accuracy\n",
    "# For TF1.14, added compile=False. This is not needed for TF2\n",
    "model = tensorflow.keras.models.load_model('/home/jmv/data/mlproj1_new/tmp/20200304-232855/00062_0.104636_0.969000_0.155657_0.948171.h5', compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tstDataGen = tensorflow.keras.preprocessing.image.ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "tstGenerator = tstDataGen.flow_from_directory(\n",
    "    directory=tstDir,\n",
    "    target_size=(imageHeight,imageWidth),\n",
    "    batch_size=1,\n",
    "    class_mode=None,\n",
    "    shuffle=False)\n",
    "\n",
    "# Confusion matrix\n",
    "predictions = model.predict_generator(tstGenerator,numpy.math.ceil(tstGenerator.samples/tstGenerator.batch_size))\n",
    "images = tstGenerator.filenames\n",
    "trueClasses = tstGenerator.classes\n",
    "predictedClasses = numpy.argmax(predictions, axis=1)\n",
    "\n",
    "report = sklearn.metrics.confusion_matrix(trueClasses, predictedClasses)\n",
    "\n",
    "print(tstGenerator.class_indices)\n",
    "print(report)\n",
    "\n",
    "# List images which have a different predicted class vs. true class\n",
    "for image, trueClass, predictedClass in zip(images,trueClasses,predictedClasses):\n",
    "    if trueClass!=predictedClass:\n",
    "        print(\\\"Image: %s, True Class: %d, Predicted Class: %d\\\" % (image, trueClass, predictedClass))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remDataGen = tensorflow.keras.preprocessing.image.ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "remGenerator = remDataGen.flow_from_directory(\n",
    "    directory=remDir,\n",
    "    target_size=(imageHeight,imageWidth),\n",
    "    batch_size=1,\n",
    "    class_mode=None,\n",
    "    shuffle=False)\n",
    "\n",
    "# Confusion matrix\n",
    "predictions = model.predict_generator(remGenerator,numpy.math.ceil(remGenerator.samples/remGenerator.batch_size))\n",
    "images = remGenerator.filenames\n",
    "trueClasses = remGenerator.classes\n",
    "predictedClasses = numpy.argmax(predictions, axis=1)\n",
    "\n",
    "report = sklearn.metrics.confusion_matrix(trueClasses, predictedClasses)\n",
    "\n",
    "print(remGenerator.class_indices)\n",
    "print(report)\n",
    "\n",
    "for image, trueClass, predictedClass in zip(images,trueClasses,predictedClasses):\n",
    "    if trueClass!=predictedClass:\n",
    "        print(\\\"Image: %s, True Class: %d, Predicted Class: %d\\\" % (image, trueClass, predictedClass))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From book \\\"Deep Learning w/ Python\\\" by FranÃ§ois Chollet\n",
    "# From https://stackoverflow.com/questions/58322147/how-to-generate-cnn-heatmaps-using-built-in-keras-in-tf2-0-tf-keras\n",
    "import matplotlib.pyplot\n",
    "import cv2\n",
    "import PIL\n",
    "\n",
    "def plot_activation(imagePath):\n",
    "    # Loads an image into PIL format\n",
    "    myImage = tensorflow.keras.preprocessing.image.load_img(imagePath,target_size=(imageHeight,imageWidth))\n",
    "    # Converts the PIL image into a Numpy array\n",
    "    myImageAsArray = tensorflow.keras.preprocessing.image.img_to_array(myImage)\n",
    "    # Creates a list containing a single image [myImageAsArray]\n",
    "    myImageAsArray = numpy.expand_dims(myImageAsArray,axis=0)\n",
    "    # Scales the image in the same way as what we did before the training\n",
    "    myImageAsArray /= 255.0\n",
    "    # Gets the result of the model    \n",
    "    myPrediction = model.predict(myImageAsArray)    \n",
    "    myPredictedClass = numpy.argmax(myPrediction, axis=1)    \n",
    "    #print(f\\\"DBG> Predicted class: {myPredictedClass[0]}\\\")    \n",
    "    #    \n",
    "    convLayer = model.get_layer(\\\"block5_conv3\\\")    \n",
    "    #print(\\\"DBG> convLayer is\\\",convLayer)    \n",
    "    modelOutput = model.output[:,myPredictedClass[0]]    \n",
    "    # Was forced to add tensorflow.cast(...,'float32') because otherwise the tensor is missing    \n",
    "    # dtype set to float32. Bug with TF1.14?    \n",
    "    #grads = tensorflow.cast(tensorflow.keras.backend.gradients(modelOutput,convLayer.output),'float32')    \n",
    "    grads = tensorflow.keras.backend.gradients(modelOutput,convLayer.output)[0]    \n",
    "    pooledGrads = tensorflow.keras.backend.mean(grads,axis=(0,1,2))    \n",
    "    iterate = tensorflow.keras.backend.function([model.input],[pooledGrads,convLayer.output[0]])    \n",
    "    pooledGradsValue, convLayerOutputValue = iterate([myImageAsArray])    \n",
    "    numberOfChannelsConvLayer = convLayer.output[0].get_shape()[2]    \n",
    "    for i in range(numberOfChannelsConvLayer):    \n",
    "        convLayerOutputValue[:,:,i] *= pooledGradsValue[i]    \n",
    "    heatMap = numpy.mean(convLayerOutputValue, axis=-1)    \n",
    "    heatMap = numpy.maximum(heatMap,0)    \n",
    "    heatMap /= numpy.max(heatMap)    \n",
    "    matplotlib.pyplot.matshow(heatMap)    \n",
    "    #    \n",
    "    img = cv2.imread(imagePath)    \n",
    "    heatMap = cv2.resize(heatMap,(img.shape[1],img.shape[0]))    \n",
    "    heatMap =numpy.uint8(255*heatMap)    \n",
    "    heatMap = cv2.applyColorMap(heatMap,cv2.COLORMAP_JET)    \n",
    "    superImposedImg = heatMap*0.4+img    \n",
    "    cv2.imwrite('/home/jmv/data/mlproj8/myresultingimage.jpg',superImposedImg)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desiredNumberOfImagesToDisplay = 10    \n",
    "dirToDisplayFrom = tstDir    \n",
    "print(f\\\"DBG> Desired number of images: {desiredNumberOfImagesToDisplay}\\\")    \n",
    "print(f\\\"DBG> Directory to display images from: {dirToDisplayFrom}\\\")    \n",
    "  \n",
    "for myClass in sorted(os.listdir(dirToDisplayFrom)):    \n",
    "    print(f\\\"DBG> Class={myClass}\\\")    \n",
    "    listOfImages = [image for image in sorted(os.listdir(os.path.join(tstDir,myClass))) if \\\"copy\\\" not in image]    \n",
    "    actualNumberOfImagesToDisplay = len(listOfImages)    \n",
    "    print(f\\\"DBG> Actual number of images: {actualNumberOfImagesToDisplay}\\\")    \n",
    "    for image in listOfImages[:min(desiredNumberOfImagesToDisplay,actualNumberOfImagesToDisplay)]:    \n",
    "        print(f\\\"DBG> Image: {image}\\\")    \n",
    "        selectImage = os.path.join(tstDir,myClass,image)    \n",
    "        plot_activation(selectImage)    \n",
    "        pil_img = PIL.Image.open('/home/jmv/data/mlproj8/myresultingimage.jpg')    \n",
    "        myImShow = matplotlib.pyplot.imshow(pil_img)    \n",
    "        matplotlib.pyplot.title(selectImage,pad=30)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
